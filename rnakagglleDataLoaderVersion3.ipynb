{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-22T08:45:51.524849Z","iopub.execute_input":"2023-10-22T08:45:51.525280Z"}}},{"cell_type":"code","source":"# Importing libraries.\nimport pandas as pd\nimport os, gc\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom fastai.vision.all import *\ndef flatten(o):\n    \"Concatenate all collections and items as a generator\"\n    for item in o:\n        if isinstance(o, dict): yield o[item]; continue\n        elif isinstance(item, str): yield item; continue\n        try: yield from flatten(item)\n        except TypeError: yield item","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler autocast\n@delegates(GradScaler)\nclass MixedPrecision(Callback):\n    \"Mixed precison training using Pytorch's 'autocast and GradScaler'\"\n    order = 10\n    def __init__(self, **kwargs): self.kwargs = kwargs\n    def before_fit(self): self.autocast, self.learn.scaler, self.scales = autocast(), GradScaler(**self.kwargs), L()\n    def before_batch(self): self.autocast.__enter__()\n    def after_pred(self):\n        if next(flatten(self.pred)).dtype==torch.float16:\n            self.learn.pred = to_float(self.pred)\n    def after_loss(self): self.autocast.__exit__(None, None, None)\n    def before_backward(self): self.learn.loss_grad = self.scaler.scale(self.loss_grad)\n    def before_step(self):\n        self.skipped = True\n        self.scaler.step(self)\n        if self.skipped: raise CancelStepException()\n        self.scales.append(self.scaler.get_scale())\n    def after_step(self): self.learn.scaler.update()\n        \n    @property\n    def param_groups(self):\n        return self.opt.param_groups\n    def step(self, *args, **kwargs):\n        self.skipped = False\n    def after_fit(self): self.autocast, self.learn.scaler, self.scales = None, None, None\n        \n\nimport fastai\nfastai.callback.fp16.MixedPrecision = MixedPrecision\n\n        \n    \n        \n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fname = 'example0'\nPATH = '/kaggle/input/stanford-ribonanza-rna-folding-converted/'\nOUT = './'\nbs = 256\nnum_workers = 2\nSEED = 2023\nnfolds = 4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n","metadata":{"execution":{"iopub.status.busy":"2023-10-22T09:04:54.614796Z","iopub.execute_input":"2023-10-22T09:04:54.615253Z","iopub.status.idle":"2023-10-22T09:04:54.622397Z","shell.execute_reply.started":"2023-10-22T09:04:54.615220Z","shell.execute_reply":"2023-10-22T09:04:54.621005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNA_Dataset(Dataset):\n    def __init__(self, df, mode='train', seed=2023, fold= 0, nfolds = 4, mask_onlt = False, **kwargs):\n        self.seq_map = {'A' : 0, 'C': 1, 'G': 2, 'U': 3}\n        self.Lmax = 206\n        df['L'] = df.sequence.apply(len)\n        df_2A3 = df.loc[df.experiment_type == '2A3_MaP']\n        df_DMS = df.loc[df.experiment_type == 'DMS_MaP']\n        split = list(KFold(n_splits = nfolds, random_state = seed,\n                          shuffle = True).split(df_2A3))[fold][0 if mode == 'train' else 1]\n        \n        df_2A3 = df_2A3.iloc[split].reset_index(drop = True)\n        df_DMS = df_DMS.iloc[split].reset_index(drop = True)\n        \n        m = (df_2A3['SN_filter'].values > 0) & (df_DMS['SN_filter'].values > 0)\n        \n        df_2A3 = df_2A3.loc[m].reset_index(drop = True)\n        df_DMS = df_DMS.loc[m].reset_index(drop = True)\n        \n        self.seq = df_2A3['sequence'].values\n        self.L = df_2A3['L'].values\n        \n        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n                                'reactivity_0' in c]].values\n        \n        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n                                'reactivity_0' in c]].values\n        self.react_err_2A3 = df_2A3[[ c for c in df_2A3.columns if \\\n                                    'reactivity_error_0' in c]].values\n        self.react_err_DMS = df_DMS[[c for c in df_DMS.columns if \\\n                                    'reactivity_error_0' in c]].values\n        self.sn_2A3 = df_2A3['signal_to_noise'].values\n        self.sn_DMS = df_DMS['signal_to_noise'].values\n        self.mask_only = mask_only\n        \n    def __len__(self):\n        return len(self.seq)\n    \n    def __getitem__(self, idx):\n        seq = self.seq[idx]\n        if self.mask_only:\n            mask = torch.zeros(self.Lmax, dtype = torch.bool)\n            mask[:len(seq)] = True\n            return {'mask': mask}, {'mask': mask}\n        seq = [self.seq_map[s] for s in seq]\n        seq = np.array(seq)\n        mask = torch.zeros(self.Lmax, dtype=torch.bool)\n        mask[:len(seq)] = True\n        seq = np.pad(seq, (0, self.Lmax - len(seq)))\n        react = torch.from_numpy(np.stack([self.react_2A3[idx], self.react_DMS[idx]], -1))\n        react_err = torch.from_numpy(np.stack([self.react_err_2A3[idx], self.react_err_DMS[idx]], -1))\n        sn = torch.FloatTensor([self.sn_2A3[idx], self.sn_DMS[idx]])\n        return {'seq': torch.from_numpy(seq), 'mask':mask}, {'react': react, 'react_err': react_err, 'sn': sn, 'mask': mask}\n    \n\nclass LenMatchBatchSampler(torch.utils.data.BatchSampler):\n        def __iter__(self):\n            buckets = [[]] * 100\n            yielded = 0\n            for idx in self.sampler:\n                s = self.sampler.data_source[idx]\n                if isinstance(s, tuple): L = s[0]['mask'].sum()\n                else: L = s['mask'].sum()\n                L = max(1, L // 16)\n                if len(buckets[L]) == 0: buckets[L] = []\n                buckets[L].append(idx)\n                \n                if len(buckets[L]) == self.batch_size:\n                    batch = list(bucktes[L])\n                    yield batch\n                    yielded += 1\n                    bucktes[L] = []\n                    \n            batch = []\n            leftover = [idx for bucket in buckets for idx in bucket]\n            \n            for idx in leftover:\n                batch.append(idx)\n                if len(batch) == self.batch_size:\n                    yielded += 1\n                    yield batch\n                    batch = []\n                    \n            if len(batch) > 0 and not self.drop_last:\n                yielded += 1\n                yield batch\n        \n        def dict_to(x, device = 'cuda'):\n            return {k:x[k].to(device) for k in x}\n        \n        def to_device(x, device = 'cuda'):\n            return tuple(dict_to(e device) for e in x)\n        \nclass DeviceDataLoader:\n        def __init__(self, dataloader, device = 'cuda'):\n            self.dataloader = dataloader\n            self.device = device\n            \n        def __len__(self):\n            return len(self.dataloader)\n        \n        def __iter__(self):\n            for batch in self.dataloader:\n                yield tuple(dict_to(x, self.device) for x in batch)\n        ","metadata":{},"execution_count":null,"outputs":[]}]}