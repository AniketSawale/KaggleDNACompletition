{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-21T16:53:36.413621Z","iopub.execute_input":"2023-10-21T16:53:36.414401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing libraries.\nimport pandas as pd\nimport os, gc\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-10-21T17:14:25.821595Z","iopub.execute_input":"2023-10-21T17:14:25.822151Z","iopub.status.idle":"2023-10-21T17:14:29.508324Z","shell.execute_reply.started":"2023-10-21T17:14:25.822119Z","shell.execute_reply":"2023-10-21T17:14:29.506316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom fastai.vision.all import *\ndef flatten(o):\n    \"Concatenate all collections and items as a generator\"\n    for item in o:\n        if isinstance(o, dict): yield o[item]; continue\n        elif isinstance(item, str): yield item; continue\n        try: yield from flatten(item)\n        except TypeError: yield item","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler autocast\n@delegates(GradScaler)\nclass MixedPrecision(Callback):\n    \"Mixed precison training using Pytorch's 'autocast and GradScaler'\"\n    order = 10\n    def __init__(self, **kwargs): self.kwargs = kwargs\n    def before_fit(self): self.autocast, self.learn.scaler, self.scales = autocast(), GradScaler(**self.kwargs), L()\n    def before_batch(self): self.autocast.__enter__()\n    def after_pred(self):\n        if next(flatten(self.pred)).dtype==torch.float16:\n            self.learn.pred = to_float(self.pred)\n    def after_loss(self): self.autocast.__exit__(None, None, None)\n    def before_backward(self): self.learn.loss_grad = self.scaler.scale(self.loss_grad)\n    def before_step(self):\n        self.skipped = True\n        self.scaler.step(self)\n        if self.skipped: raise CancelStepException()\n        self.scales.append(self.scaler.get_scale())\n    def after_step(self): self.learn.scaler.update()\n        \n    @property\n    def param_groups(self):\n        return self.opt.param_groups\n    def step(self, *args, **kwargs):\n        self.skipped = False\n    def after_fit(self): self.autocast, self.learn.scaler, self.scales = None, None, None\n        \n\nimport fastai\nfastai.callback.fp16.MixedPrecision = MixedPrecision\n\n        \n    \n        \n        \n","metadata":{},"execution_count":null,"outputs":[]}]}